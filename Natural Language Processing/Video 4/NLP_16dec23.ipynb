{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMMAce5eAWqiTC+MAT1xy/U"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# NLP Live Class - PWSkills\n","# Mentor - Dr Ayan Debnath\n","## class on 16th Dec 2023"],"metadata":{"id":"ATSksihWu2YA"}},{"cell_type":"markdown","source":["# BOW and TF-IDF implementation"],"metadata":{"id":"6Ry8G916A5TP"}},{"cell_type":"code","source":["# how to find unique words in your corpus\n","# Vocab\n","\n","corpus = ['This is the first sentence in our corpus followed by one more sentence to demonstrate Bag of Words',\n","          'This is the sencond sentence in our corpus with a FEW UPPER WORDS and Few Title Case Words',\n","          'this is the third sentence in our corpus']\n","\n","vocab = []\n","total_words = 0\n","\n","for sentence in corpus:\n","  sentence = sentence.lower()\n","  token_temp = sentence.split()\n","  total_words = total_words + len(token_temp)\n","  for i in range(len(token_temp)):\n","    if token_temp[i] not in vocab:\n","      vocab.append(token_temp[i])\n","\n","vocab.sort()\n","print(vocab)\n","print('There are {} words in vocabulary.'.format(len(vocab)))\n","print('A total of {} words in corpus.'.format(total_words))\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gH9M-OJmArRu","executionInfo":{"status":"ok","timestamp":1702739942668,"user_tz":-330,"elapsed":8,"user":{"displayName":"Ayan Debnath","userId":"06573521543200295119"}},"outputId":"1c07ab4b-6843-4a21-97ac-f2b1ed9351d0"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["['a', 'and', 'bag', 'by', 'case', 'corpus', 'demonstrate', 'few', 'first', 'followed', 'in', 'is', 'more', 'of', 'one', 'our', 'sencond', 'sentence', 'the', 'third', 'this', 'title', 'to', 'upper', 'with', 'words']\n","There are 26 words in vocabulary.\n","A total of 44 words in corpus.\n"]}]},{"cell_type":"code","source":["data = [ \"I love Natural language Processing\",\n","        \"creating word vectors\",\n","         \"Is my jam!\" ]\n","\n","\n","\n","# BOW\n","from sklearn.feature_extraction.text import CountVectorizer\n","cv = CountVectorizer()\n","X_bow = cv.fit_transform(data).toarray()\n","print(\"BOW:\", X_bow)\n","\n","#TF_IDF\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","# fit\n","tfvectorizer = TfidfVectorizer()\n","X_tfidf = tfvectorizer.fit_transform(data).toarray()\n","print(\"TF_IDF:\",X_tfidf)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WGTWgarpArZ3","executionInfo":{"status":"ok","timestamp":1702743580903,"user_tz":-330,"elapsed":338,"user":{"displayName":"Ayan Debnath","userId":"06573521543200295119"}},"outputId":"ea8434c7-0136-41e0-c11a-0d5044e2a383"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["BOW: [[0 0 0 1 1 0 1 1 0 0]\n"," [1 0 0 0 0 0 0 0 1 1]\n"," [0 1 1 0 0 1 0 0 0 0]]\n","TF_IDF: [[0.         0.         0.         0.5        0.5        0.\n","  0.5        0.5        0.         0.        ]\n"," [0.57735027 0.         0.         0.         0.         0.\n","  0.         0.         0.57735027 0.57735027]\n"," [0.         0.57735027 0.57735027 0.         0.         0.57735027\n","  0.         0.         0.         0.        ]]\n"]}]},{"cell_type":"code","source":["X = vectorized_data.toarray()\n","X"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NtZLhMhTArc7","executionInfo":{"status":"ok","timestamp":1702743267809,"user_tz":-330,"elapsed":572,"user":{"displayName":"Ayan Debnath","userId":"06573521543200295119"}},"outputId":"077cb48f-37fa-496b-9160-e3cd5363a210"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.        , 0.        , 0.        , 0.5       , 0.5       ,\n","        0.        , 0.5       , 0.5       , 0.        , 0.        ],\n","       [0.57735027, 0.        , 0.        , 0.        , 0.        ,\n","        0.        , 0.        , 0.        , 0.57735027, 0.57735027],\n","       [0.        , 0.57735027, 0.57735027, 0.        , 0.        ,\n","        0.57735027, 0.        , 0.        , 0.        , 0.        ]])"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\n","cv = CountVectorizer()\n","X_bow = cv.fit_transform(data).toarray()\n","X_bow"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"72kRHcUHArf6","executionInfo":{"status":"ok","timestamp":1702743376168,"user_tz":-330,"elapsed":9,"user":{"displayName":"Ayan Debnath","userId":"06573521543200295119"}},"outputId":"016e97ec-0b76-4f8f-90a3-183e9fde5c54"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0, 0, 0, 1, 1, 0, 1, 1, 0, 0],\n","       [1, 0, 0, 0, 0, 0, 0, 0, 1, 1],\n","       [0, 1, 1, 0, 0, 1, 0, 0, 0, 0]])"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":[],"metadata":{"id":"QvA8Ut_dAri0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"og9DjJMVArlE"},"execution_count":null,"outputs":[]}]}